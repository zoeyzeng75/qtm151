---
title: QTM 151 - Intro to Statistical Programming II
subtitle: Lecture 13 - Replacing and Recoding Variables
date: 2024-10-14
date-format: "DD MMMM, YYYY"
author:
  - name: Danilo Freire
    email: danilo.freire@emory.edu
    affiliations: Emory University
format:
  clean-revealjs:
    self-contained: true
    footer: "[Variables](https://raw.githack.com/danilofreire/qtm151/main/lectures/lecture-13/13-replacing-recoding-variables.html)"
    drop:
      shortcut: "`"
      button: true
      engine: pyodide
      pyodide:
        packages:
          - matplotlib
          - numpy
          - pandas
transition: slide
transition-speed: default
scrollable: true
engine: jupyter
revealjs-plugins:
  - drop
  - fontawesome
filters:
  - pyodide
editor:
  render-on-save: true
---

# Nice to see you again! How was your break? 🍂 {background-color="#2d4563"}

# Today's plan 📅 {background-color="#2d4563"}

## Creating and replacing variables
### Today we will...

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- Learn how to recode and replace variables in a dataset
- Specifically focus on replacing `NaN` values ("Not a Number" - missing data)
- Cover how to convert variables from one type to another
- Learn how to create new variables based on existing ones
- We will use our friends `pandas` and `numpy` to do this 🐼
:::

:::{.column width="50%"}
:::{style="text-align: center;"}
![](figures/recode.png)
:::
:::
:::
:::

# Loading packages and dataset 📦 {background-color="#2d4563"}

## Our dataset: Formula 1 World Championships 🏁🏎️

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- First, we will load the packages we need

```{python}
#| echo: true
#| eval: true
import pandas as pd
import numpy as np
```

- Then, we will load the dataset

```{python}
#| echo: true
#| eval: true
circuits = pd.read_csv("https://github.com/danilofreire/qtm151/raw/refs/heads/main/lectures/lecture-13/data_raw/circuits.csv")

# Or open it from the local file
# circuits = pd.read_csv("data_raw/circuits.csv")

display(circuits.head(2))
```
:::

:::{.column width="50%"}
- The dataset contains information about F1 circuits, such as its name, location, latitude, longitude, and more
- You can find more information about the dataset [here](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020/data)
- The dataset is available in the course's GitHub repository [here](https://github.com/danilofreire/qtm151/blob/main/lectures/lecture-13/data_raw/circuits.csv)
  - Or you can download it using the command above
- Let's see how the codebook looks like
- More information about [Formula 1 here](https://en.wikipedia.org/wiki/Formula_One)
:::
:::
:::

## Codebook 📚

:::{style="margin-top: 30px; font-size: 23px;"}
:::{.columns}
:::{.column width="50%"}
![](figures/codebook.png)

- `Field` - Name of the variable
- `Type` - Type of the variable
  - Integer (`int`), string (`str` - `varchart`), and float (`float`)
- `Description` - Label with a description of the variable
- [Quick discussion]{.alert}: What does `varchart(255)` mean?
:::

:::{.column width="50%"}
- The dataset has `{python} circuits.shape[1]` columns (variables) and `{python} circuits.shape[0]`  rows (observations)
- The columns are:
  - `circuitId`: Unique identifier for the circuit
  - `circuitRef`: Unique reference for the circuit
  - `name`: Name of the circuit
  - `location`: Location 
  - `country`: Country where the circuit is located
  - `lat`: Latitude 
  - `lng`: Longitude
  - `alt`: Altitude
  - `url`: URL of the circuit's Wikipedia page
:::
:::
:::

# NaN values 🚫 {background-color="#2d4563"}

## What is a `NaN` value?

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- `NaN` stands for "Not a Number"
- It is a special value in Python that represents missing data
- `NaN` values can be found in datasets for various reasons
  - Data entry errors
  - Data cleaning and processing errors
  - Data collection errors
  - Data transformation errors
- We (often) need to handle `NaN` values before we can analyse the data
:::

:::{.column width="50%"}
- `NaN` values can be found in different types of variables
  - Numeric variables
  - Categorical variables
  - Date variables
  - Text variables
- We will focus on numeric variables today
- `pandas` and `numpy` have functions to handle `NaN` values
  - Note: they handle `NaN` values differently!
:::
:::
:::

## Operations with `NaN` values

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- `NaN` is a special number, available in `numpy`

```{python}
#| echo: true
#| eval: true
np.nan
```

- Often, we cannot perform operations with `NaN` values
- Thus, we need to handle them before we can analyse the data
:::

:::{.column width="50%"}
- Let's see some examples. We start with `numpy` arrays

```{python}
#| echo: true
#| eval: true
# Create two array with and without "NaNs"
# The "np.array()" functions converts 
# a list to an array

vec_without_nans = np.array([1,1,1])
vec_with_nans    = np.array([np.nan,4,5])

# When you add the vectors
# you will produce a NaN 
# on any entries with "NaNs"
print(vec_without_nans * vec_with_nans)
print(vec_without_nans / vec_with_nans)
print(vec_without_nans + vec_with_nans)
print(vec_without_nans - vec_with_nans)
```
:::
:::
:::

## Summary statistics with `NaN` values
### Arrays

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- Some summary statistics functions will not work with `NaN` values
- For example, the `mean()` function

```{python}
#| echo: true
#| eval: true
print(np.mean(vec_with_nans))
```

- The `mean()` function will return `NaN` if there are `NaN` values in the array
:::

:::{.column width="50%"}
- To calculate the mean without `NaN` values, we can use the `nanmean()` function

```{python}
#| echo: true
#| eval: true
print(np.nanmean(vec_with_nans))
```

- The `nanmean()` function will ignore `NaN` values and calculate the mean with the remaining values
:::
:::
:::

## Summary statistics with `NaN` values
### Pandas DataFrames

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- Let's create an empty DataFrame and create a new column `x` with `NaN` values

```{python}
#| echo: true
#| eval: true
dataset = pd.DataFrame()
dataset["x"] = vec_with_nans
dataset
```
:::

:::{.column width="50%"}
- You will see that `pandas` will handle `NaN` values differently: it will [ignore them]{.alert}

```{python}
#| echo: true
#| eval: true
print(dataset["x"].mean())
```

- [For R users]{.alert}: This is the same as `na.rm = TRUE` in R. `pandas` does that by default
:::
:::
:::

# Data Cleaning 🧹🧽 {background-color="#2d4563"}

## Data cleaning

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- Data cleaning is the process of preparing data for analysis
- It involves identifying and handling missing data, outliers, and other data quality issues
- [You guys have no idea]{.alert} how much time you will spend cleaning data in your life 😅
- According to a [Forbes survey](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/), data scientists spend 60% of their time cleaning and preparing data, and 57% say it's the least enjoyable part of their work
  - I can [really]{.alert} relate to that 😂
- But remember that [clean data are good data]{.alert} 🥳
:::

:::{.column width="50%"}
- Let's get the data types of the columns in the `circuits` dataset
- We use the command `dtypes` for that
- `object` means that the variable is a string or a variable with mixed types (e.g., numbers and strings)

```{python}
#| echo: true
#| eval: true
circuits.dtypes
```
:::
:::
:::

## Check rows with numeric values

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- Here we will use the `.str.isnumeric()` function
- This function actually combines two functions: `.str` and `.isnumeric()`
- The `.str` function is used to check if the variable is a string
- The `.isnumeric()` part is used to check if the string is numeric
- [Why do we need both?]{.alert} Because DataFrame columns often contain mixed data types (e.g., numbers and strings), and we need to check if the variable is a string before we can check if it is numeric
- If we used only `.isnumeric()`, we would get an error (trust me, I tried 😅)
:::

:::{.column width="50%"}
- The two dots between the functions are called [method chaining]{.alert}
- It is a way to call multiple functions in a single line of code
- If you use `R`, this is similar to the `%>%` operator in `dplyr`
- Let's see how it works

```{python}
#| echo: true
#| eval: true
# Check if the variable "alt" is numeric
circuits["alt"].str.isnumeric()
```
:::
:::
:::

## Other examples of chaining methods

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
```{python}
#| echo: true
#| eval: true
# Check if the variable "circuitRef" is numeric
circuits["circuitRef"].str.isnumeric()
```
:::

:::{.column width="50%"}
```{python}
#| echo: true
#| eval: true
# Convert the variable `location` to lowercase
circuits["location"].str.lower()
```
:::
:::
:::

## Extract list of non-numeric values

:::{style="margin-top: 30px; font-size: 25px;"}
- Remember `.query()`? We can use it to reference subattributes of a variable
- Here we will combine it with `pd.unique()` to extract a list of non-numeric values
- The `pd.unique()` function will return a list of unique values in a variable

```{python}
#| echo: true
#| eval: true
# Extract a list of non-numeric values
# The pd.unique() function extracts unique values from a list
# Check each value in the alt column to see if it is not numeric
# True if it is not numeric, False if it is numeric
subset = circuits.query("alt.str.isnumeric() == False")
list_unique = pd.unique(subset["alt"])
print(list_unique)
```
:::

## Replace certain values

:::{style="margin-top: 30px; font-size: 25px;"}
- The `replace` function is used to replace values in a variable
- The syntax is `dataframe["variable"].replace(list_old, list_new)`
- More information about the function can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html)
```{python}
#| echo: true
#| eval: true
# "list_old" encodes values we want to change
# From the list_unique, we see that the values we want to change are '\N' and '-7'
# "list_new" encodes the values that will replace the old
list_old = ['\\N','-7']
list_new = [np.nan, -7]

# This command replaces the values of the "alt" column
circuits["alt"] = circuits["alt"].replace(list_old, list_new)
```

- After the cleaning process is done, you may want to store the dataset again
- It's [strongly recommended]{.alert} to do this in a separate file from the original
- Use `to_csv()` to save the dataset as a `.csv` file

```{python}
#| echo: true
#| eval: false
circuits.to_csv("data_clean/circuits_clean.csv", index=False)
```
:::

## Try it yourself! 🧠 {#sec:exercise01}

:::{style="margin-top: 30px; font-size: 25px;"}
- Use `.replace()` with the "country" column
- Replace "UK" with "United Kingdom"
- [[Appendix 01]{.button}](#sec:appendix01)
:::

## Try it yourself! 🧠 {#sec:exercise02}

:::{style="margin-top: 30px; font-size: 25px;"}
- What is the column type of "lat" or "lng"?
- Does it have any string variables?
- Can we use ```str.isnumeric()``` here?
:::

# Recoding Numeric Variables 🔄 {background-color="#2d4563"}

## Recoding numeric variables

:::{style="margin-top: 30px; font-size: 25px;"}
:::{.columns}
:::{.column width="50%"}
- Recoding is the process of changing the values of a variable
- We can recode variables for various reasons
  - To create new variables
  - To standardise variables
  - To simplify the analysis
- Please remember to convert the variable to the correct type before recoding

```{python}
#| echo: true
#| eval: true
# Check the data type of the "alt" column
circuits["alt"].dtype
```
:::


:::{.column width="50%"}
- `pd.to_numeric()` is used to convert a variable to a numeric type
```{python}
#| echo: true
#| eval: true
# pd.to_numeric() converts 
# a column to numeric
# Before you use this option, 
# make sure to "clean" the variable
# as we did before by checking what
# the non-numeric values are
circuits["alt_numeric"] = pd.to_numeric(circuits["alt"])
print(circuits["alt_numeric"].mean())
```

```{python}
#| echo: true
#| eval: true
print(circuits["alt_numeric"].min())
print(circuits["alt_numeric"].max())
```
:::
:::
:::

## Recode variables based on an interval {#sec:recoding}

:::{style="margin-top: 30px; font-size: 25px;"}
- Imagine that we want to recode the `alt` variable into an interval

$$x_{bin} = \begin{cases} "A" &\text{ if } x_1 < x \le x_2 \\
                                  "B" &\text{ if } x_2 < x \le x_3 \end{cases} $$

- We can use the `pd.cut()` function to do this
- The syntax is `df["new_variable"] = pd.cut(df["variable"], bins = [x1, x2, x3], labels = ["A", "B"])`
- Where `bins` are the intervals and `labels` are the new values
- More information about the function can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html)

```{python}
#| echo: true
#| eval: true
# Recode the "alt" variable into an interval
bins_x = [0, 2500, 5000]
labels_x = ["Between 0 and 2500",
            "Between 2500 and 5000"]

circuits["bins_alt"] = pd.cut(circuits["alt_numeric"],
                              bins = bins_x,
                              right = True,
                              labels = labels_x)
np.random.seed(2014)
display(circuits.sample(5))
```
:::

## Try it yourself! 🧠 {#sec:exercise03}

:::{style="margin-top: 30px; font-size: 25px;"}
- Create a new variable "hemisphere"
- Encode lattitude in (-90 and 0] as "south"
- Encode lattitude in (0 and 90] as "north"
- [[Appendix 03]{.button}](#sec:appendix03)
:::

# And that's it for today! 🎉 {background-color="#2d4563"}

# See you next time! 🚀 {background-color="#2d4563"}

## Appendix 01 {#sec:appendix01}

:::{style="margin-top: 30px; font-size: 25px;"}
```{python}
#| echo: true
#| eval: true
# Replace "UK" with "United Kingdom"
circuits["country"] = circuits["country"].replace("UK", "United Kingdom")

# Check the rows where the replacement was made
uk = circuits.query("country == 'United Kingdom'")
display(uk.head(5))
```

[[Back to exercise]{.button}](#sec:exercise01)
:::

## Appendix 02 {#sec:appendix02}

:::{style="margin-top: 30px; font-size: 25px;"}
```{python}
#| echo: true
#| eval: true
# Check the data type of the "lat" column
print(circuits["lat"].dtype)
```

```{python}
#| echo: true
#| eval: true
# Check if the "lat" column has any string values
# print(circuits["lat"].str.isnumeric())
# No, it gives an error!
 
# This one doesn't work either
# print(circuits.query("lat.str.isnumeric() == False"))

# We can use the following command to check if the column has any string values
print(circuits["lat"].dtype == 'string')
```

[[Back to exercise]{.button}](#sec:exercise02)
:::

## Appendix 03 {#sec:appendix03}

:::{style="margin-top: 30px; font-size: 25px;"}
```{python}
#| echo: true
#| eval: true
# Create a new variable "hemisphere"
# Encode lattitude in (-90 and 0] as "south"
# Encode lattitude in (0 and 90] as "north"
bins_lat = [-90, 0, 90]
labels_lat = ["south", "north"]

circuits["hemisphere"] = pd.cut(circuits["lat"],
                                bins = bins_lat,
                                right = True,
                                labels = labels_lat)
np.random.seed(151)
display(circuits.sample(5))
```

[[Back to exercise]{.button}](#sec:exercise03)
:::